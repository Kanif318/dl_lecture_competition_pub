【1/50】
train time: 668.93 [s]
train loss: 6.4119
train acc: 0.4427
train simple acc: 0.3551
【2/50】
train time: 562.19 [s]
train loss: 5.9303
train acc: 0.4716
train simple acc: 0.3798
【3/50】
train time: 572.31 [s]
train loss: 5.7982
train acc: 0.4723
train simple acc: 0.3807
【4/50】
train time: 584.77 [s]
train loss: 5.6949
train acc: 0.4722
train simple acc: 0.3803
【5/50】
train time: 584.66 [s]
train loss: 5.6301
train acc: 0.4722
train simple acc: 0.3805
【6/50】
train time: 581.29 [s]
train loss: 5.5814
train acc: 0.4723
train simple acc: 0.3799
【7/50】
train time: 577.46 [s]
train loss: 5.5224
train acc: 0.4729
train simple acc: 0.3811
【8/50】
train time: 565.45 [s]
train loss: 5.4667
train acc: 0.4715
train simple acc: 0.3798
Traceback (most recent call last):
  File "C:\Users\uta_f\Project\dl_lecture_competition_pub\main.py", line 537, in <module>
    main()
  File "C:\Users\uta_f\Project\dl_lecture_competition_pub\main.py", line 502, in main
    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device, train_dataset)
  File "C:\Users\uta_f\Project\dl_lecture_competition_pub\main.py", line 391, in train
    for image, question, answers, mode_answer in dataloader:
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\utils\data\dataloader.py", line 631, in __next__
    data = self._next_data()
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\utils\data\dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\utils\data\_utils\fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\utils\data\_utils\fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\uta_f\Project\dl_lecture_competition_pub\main.py", line 165, in __getitem__
    image = self.transform(image)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torchvision\transforms\transforms.py", line 354, in forward
    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torchvision\transforms\functional.py", line 468, in resize
    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\torchvision\transforms\_functional_pil.py", line 250, in resize
    return img.resize(tuple(size[::-1]), interpolation)
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\PIL\Image.py", line 2293, in resize
    self.load()
  File "C:\Users\uta_f\.virtualenvs\dl_lecture_competition_pub-JWry4h2n\lib\site-packages\PIL\ImageFile.py", line 293, in load
    n, err_code = decoder.decode(b)
KeyboardInterrupt